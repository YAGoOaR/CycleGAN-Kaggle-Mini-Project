{"cells":[{"cell_type":"markdown","metadata":{},"source":["## CycleGAN Kaggle mini project\n","\n","[Link to Kaggle competition](https://www.kaggle.com/competitions/gan-getting-started/overview)<br>\n","[Link to my repository](https://github.com/YAGoOaR/CycleGAN-Kaggle-Mini-Project)\n","\n","### Step 1 Problem description\n","We have a problem of image to image translation where we should add Monet style to our images.<br>\n","Our input is 256x256 pixel size. And we have both independent sets of real photos and monet paintings. "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2022-08-27T17:08:04.386606Z","iopub.status.busy":"2022-08-27T17:08:04.386227Z","iopub.status.idle":"2022-08-27T17:08:04.395632Z","shell.execute_reply":"2022-08-27T17:08:04.394753Z","shell.execute_reply.started":"2022-08-27T17:08:04.386569Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow_addons as tfa\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from kaggle_datasets import KaggleDatasets\n","\n","try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    print('Device:', tpu.master())\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","except:\n","    strategy = tf.distribute.get_strategy()\n","print('Number of replicas:', strategy.num_replicas_in_sync)\n","\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","    \n","print(tf.__version__)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Step 2 Exploratory data analysis\n","We have 300 Monet paintings and 7038 real photos in the dataset.<br>\n","Let's see what we have:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-27T17:08:04.397651Z","iopub.status.busy":"2022-08-27T17:08:04.397177Z","iopub.status.idle":"2022-08-27T17:08:04.412688Z","shell.execute_reply":"2022-08-27T17:08:04.411470Z","shell.execute_reply.started":"2022-08-27T17:08:04.397618Z"},"trusted":true},"outputs":[],"source":["GCS_PATH = KaggleDatasets().get_gcs_path()\n","\n","MONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\n","print('Monet TFRecord Files:', len(MONET_FILENAMES))\n","\n","PHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))\n","print('Photo TFRecord Files:', len(PHOTO_FILENAMES))"]},{"cell_type":"markdown","metadata":{},"source":["This code will have the next output:<br>\n","![An image](images/img1.jpg)"]},{"cell_type":"markdown","metadata":{},"source":["We also need functions to load and transform the input images."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-27T17:08:04.414373Z","iopub.status.busy":"2022-08-27T17:08:04.413989Z","iopub.status.idle":"2022-08-27T17:08:04.423480Z","shell.execute_reply":"2022-08-27T17:08:04.422481Z","shell.execute_reply.started":"2022-08-27T17:08:04.414332Z"},"trusted":true},"outputs":[],"source":["IMAGE_SIZE = [256, 256]\n","\n","def decode_image(image):\n","    image = tf.image.decode_jpeg(image, channels=3)\n","    image = (tf.cast(image, tf.float32) / 127.5) - 1\n","    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n","    return image\n","\n","def read_tfrecord(example):\n","    tfrecord_format = {\n","        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n","        \"image\": tf.io.FixedLenFeature([], tf.string),\n","        \"target\": tf.io.FixedLenFeature([], tf.string)\n","    }\n","    example = tf.io.parse_single_example(example, tfrecord_format)\n","    image = decode_image(example['image'])\n","    return image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-27T17:08:04.425573Z","iopub.status.busy":"2022-08-27T17:08:04.425166Z","iopub.status.idle":"2022-08-27T17:08:04.433931Z","shell.execute_reply":"2022-08-27T17:08:04.433226Z","shell.execute_reply.started":"2022-08-27T17:08:04.425532Z"},"trusted":true},"outputs":[],"source":["def load_dataset(filenames, labeled=True, ordered=False):\n","    dataset = tf.data.TFRecordDataset(filenames)\n","    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-27T17:08:04.437680Z","iopub.status.busy":"2022-08-27T17:08:04.437243Z","iopub.status.idle":"2022-08-27T17:08:04.735246Z","shell.execute_reply":"2022-08-27T17:08:04.734289Z","shell.execute_reply.started":"2022-08-27T17:08:04.437643Z"},"trusted":true},"outputs":[],"source":["monet_ds = load_dataset(MONET_FILENAMES, labeled=True).batch(1)\n","photo_ds = load_dataset(PHOTO_FILENAMES, labeled=True).batch(1)"]},{"cell_type":"markdown","metadata":{},"source":["The next methods are layers that decrease and increase image sizze using convolutions with stride 2."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-27T17:08:05.225107Z","iopub.status.busy":"2022-08-27T17:08:05.224787Z","iopub.status.idle":"2022-08-27T17:08:05.232216Z","shell.execute_reply":"2022-08-27T17:08:05.231368Z","shell.execute_reply.started":"2022-08-27T17:08:05.225076Z"},"trusted":true},"outputs":[],"source":["OUTPUT_CHANNELS = 3\n","\n","def downsample(filters, size, apply_instancenorm=True):\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n","\n","    result = keras.Sequential()\n","    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n","                             kernel_initializer=initializer, use_bias=False))\n","\n","    if apply_instancenorm:\n","        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n","\n","    result.add(layers.LeakyReLU())\n","\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-27T17:08:05.233695Z","iopub.status.busy":"2022-08-27T17:08:05.233245Z","iopub.status.idle":"2022-08-27T17:08:05.241767Z","shell.execute_reply":"2022-08-27T17:08:05.240957Z","shell.execute_reply.started":"2022-08-27T17:08:05.233660Z"},"trusted":true},"outputs":[],"source":["def upsample(filters, size, apply_dropout=False):\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n","\n","    result = keras.Sequential()\n","    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n","                                      padding='same',\n","                                      kernel_initializer=initializer,\n","                                      use_bias=False))\n","\n","    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n","\n","    if apply_dropout:\n","        result.add(layers.Dropout(0.5))\n","\n","    result.add(layers.ReLU())\n","\n","    return result"]},{"cell_type":"markdown","metadata":{},"source":["### Step 3 Architecture\n","Let's build an architecture of our model.\n","\n","At first, I tried to use ([the default CycleGAN architecture in Keras documentation example](https://keras.io/examples/generative/cyclegan/)). <br>\n","But that model trains really slow. Too slow to experiment with architecture.<b>\n","We will change the CycleGAN's architecture to have better results and training time.<br>\n","\n","After experimenting with hyperparameters, I made a conclusion that we should increase epoch size to have more Monet-styled output(because generator better learns how to make styled photos).<br>\n","But the network trains very slow now, so it will train really long if I just increase epochs.<br>\n","I decided to change generator layer count to reduce the complexity of our network. Also I changed kernel count and reduced kernel size.<br>\n","Also I changed optimizer parameters to increase training speed.<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-27T17:08:05.243561Z","iopub.status.busy":"2022-08-27T17:08:05.243133Z","iopub.status.idle":"2022-08-27T17:08:05.255556Z","shell.execute_reply":"2022-08-27T17:08:05.254777Z","shell.execute_reply.started":"2022-08-27T17:08:05.243530Z"},"trusted":true},"outputs":[],"source":["def Generator():\n","    inputs = layers.Input(shape=[256,256,3])\n","\n","    # bs = batch size\n","    down_stack = [\n","        downsample(64, 3, apply_instancenorm=False), # (bs, 128, 128, 64)\n","        downsample(128, 3), # (bs, 64, 64, 128)\n","        downsample(128, 3), # (bs, 32, 32, 256)\n","        downsample(128, 3), # (bs, 16, 16, 512)\n","        downsample(128, 3), # (bs, 8, 8, 512)\n","        downsample(128, 3), # (bs, 4, 4, 512)\n","    ]\n","\n","    up_stack = [\n","        upsample(128, 3, apply_dropout=True), # (bs, 8, 8, 1024)\n","        upsample(128, 3, apply_dropout=True), # (bs, 16, 16, 1024)\n","        upsample(128, 3, apply_dropout=True), # (bs, 32, 32, 1024)\n","        upsample(128, 3), # (bs, 64, 64, 256)\n","        upsample(64, 3), # (bs, 128, 128, 128)\n","    ]\n","\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n","                                  strides=2,\n","                                  padding='same',\n","                                  kernel_initializer=initializer,\n","                                  activation='tanh') # (bs, 256, 256, 3)\n","\n","    x = inputs\n","\n","    # Downsampling through the model\n","    skips = []\n","    for down in down_stack:\n","        x = down(x)\n","        skips.append(x)\n","\n","    skips = reversed(skips[:-1])\n","\n","    # Upsampling and establishing the skip connections\n","    for up, skip in zip(up_stack, skips):\n","        x = up(x)\n","        x = layers.Concatenate()([x, skip])\n","\n","    x = last(x)\n","\n","    return keras.Model(inputs=inputs, outputs=x)"]},{"cell_type":"markdown","metadata":{},"source":["Descriminator does not seem to be very complex(and to use much computational power), so let's leave it as it is now."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-27T17:08:05.257064Z","iopub.status.busy":"2022-08-27T17:08:05.256611Z","iopub.status.idle":"2022-08-27T17:08:05.269263Z","shell.execute_reply":"2022-08-27T17:08:05.268515Z","shell.execute_reply.started":"2022-08-27T17:08:05.257022Z"},"trusted":true},"outputs":[],"source":["def Discriminator():\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n","\n","    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n","\n","    x = inp\n","\n","    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n","    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n","    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n","\n","    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n","    conv = layers.Conv2D(512, 4, strides=1,\n","                         kernel_initializer=initializer,\n","                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n","\n","    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n","\n","    leaky_relu = layers.LeakyReLU()(norm1)\n","\n","    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n","\n","    last = layers.Conv2D(1, 4, strides=1,\n","                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n","\n","    return tf.keras.Model(inputs=inp, outputs=last)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-27T17:08:05.270703Z","iopub.status.busy":"2022-08-27T17:08:05.270286Z","iopub.status.idle":"2022-08-27T17:08:09.352706Z","shell.execute_reply":"2022-08-27T17:08:09.351737Z","shell.execute_reply.started":"2022-08-27T17:08:05.270673Z"},"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    monet_generator = Generator()\n","    photo_generator = Generator()\n","\n","    monet_discriminator = Discriminator()\n","    photo_discriminator = Discriminator()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-27T17:08:09.971620Z","iopub.status.busy":"2022-08-27T17:08:09.971247Z","iopub.status.idle":"2022-08-27T17:08:09.991691Z","shell.execute_reply":"2022-08-27T17:08:09.990963Z","shell.execute_reply.started":"2022-08-27T17:08:09.971590Z"},"trusted":true},"outputs":[],"source":["class CycleGan(keras.Model):\n","    def __init__(\n","        self,\n","        monet_generator,\n","        photo_generator,\n","        monet_discriminator,\n","        photo_discriminator,\n","        lambda_cycle=10,\n","    ):\n","        super(CycleGan, self).__init__()\n","        self.m_gen = monet_generator\n","        self.p_gen = photo_generator\n","        self.m_disc = monet_discriminator\n","        self.p_disc = photo_discriminator\n","        self.lambda_cycle = lambda_cycle\n","        \n","    def compile(\n","        self,\n","        m_gen_optimizer,\n","        p_gen_optimizer,\n","        m_disc_optimizer,\n","        p_disc_optimizer,\n","        gen_loss_fn,\n","        disc_loss_fn,\n","        cycle_loss_fn,\n","        identity_loss_fn\n","    ):\n","        super(CycleGan, self).compile()\n","        self.m_gen_optimizer = m_gen_optimizer\n","        self.p_gen_optimizer = p_gen_optimizer\n","        self.m_disc_optimizer = m_disc_optimizer\n","        self.p_disc_optimizer = p_disc_optimizer\n","        self.gen_loss_fn = gen_loss_fn\n","        self.disc_loss_fn = disc_loss_fn\n","        self.cycle_loss_fn = cycle_loss_fn\n","        self.identity_loss_fn = identity_loss_fn\n","        \n","    def train_step(self, batch_data):\n","        real_monet, real_photo = batch_data\n","        \n","        with tf.GradientTape(persistent=True) as tape:\n","            # photo to monet back to photo\n","            fake_monet = self.m_gen(real_photo, training=True)\n","            cycled_photo = self.p_gen(fake_monet, training=True)\n","\n","            # monet to photo back to monet\n","            fake_photo = self.p_gen(real_monet, training=True)\n","            cycled_monet = self.m_gen(fake_photo, training=True)\n","\n","            # generating itself\n","            same_monet = self.m_gen(real_monet, training=True)\n","            same_photo = self.p_gen(real_photo, training=True)\n","\n","            # discriminator used to check, inputing real images\n","            disc_real_monet = self.m_disc(real_monet, training=True)\n","            disc_real_photo = self.p_disc(real_photo, training=True)\n","\n","            # discriminator used to check, inputing fake images\n","            disc_fake_monet = self.m_disc(fake_monet, training=True)\n","            disc_fake_photo = self.p_disc(fake_photo, training=True)\n","\n","            # evaluates generator loss\n","            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n","            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n","\n","            # evaluates total cycle consistency loss\n","            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n","\n","            # evaluates total generator loss\n","            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n","            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n","\n","            # evaluates discriminator loss\n","            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n","            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n","\n","        # Calculate the gradients for generator and discriminator\n","        monet_generator_gradients = tape.gradient(total_monet_gen_loss,\n","                                                  self.m_gen.trainable_variables)\n","        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\n","                                                  self.p_gen.trainable_variables)\n","\n","        monet_discriminator_gradients = tape.gradient(monet_disc_loss,\n","                                                      self.m_disc.trainable_variables)\n","        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\n","                                                      self.p_disc.trainable_variables)\n","\n","        # Apply the gradients to the optimizer\n","        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,\n","                                                 self.m_gen.trainable_variables))\n","\n","        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\n","                                                 self.p_gen.trainable_variables))\n","\n","        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,\n","                                                  self.m_disc.trainable_variables))\n","\n","        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n","                                                  self.p_disc.trainable_variables))\n","        \n","        return {\n","            \"monet_gen_loss\": total_monet_gen_loss,\n","            \"photo_gen_loss\": total_photo_gen_loss,\n","            \"monet_disc_loss\": monet_disc_loss,\n","            \"photo_disc_loss\": photo_disc_loss\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-27T17:08:09.994785Z","iopub.status.busy":"2022-08-27T17:08:09.994498Z","iopub.status.idle":"2022-08-27T17:08:10.017602Z","shell.execute_reply":"2022-08-27T17:08:10.016689Z","shell.execute_reply.started":"2022-08-27T17:08:09.994757Z"},"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    def discriminator_loss(real, generated):\n","        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n","\n","        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n","\n","        total_disc_loss = real_loss + generated_loss\n","\n","        return total_disc_loss * 0.5"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-27T17:08:10.019309Z","iopub.status.busy":"2022-08-27T17:08:10.018760Z","iopub.status.idle":"2022-08-27T17:08:10.029226Z","shell.execute_reply":"2022-08-27T17:08:10.027970Z","shell.execute_reply.started":"2022-08-27T17:08:10.019276Z"},"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    def generator_loss(generated):\n","        return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-27T17:08:10.031185Z","iopub.status.busy":"2022-08-27T17:08:10.030583Z","iopub.status.idle":"2022-08-27T17:08:10.039841Z","shell.execute_reply":"2022-08-27T17:08:10.038985Z","shell.execute_reply.started":"2022-08-27T17:08:10.031141Z"},"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n","        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n","\n","        return LAMBDA * loss1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-27T17:08:10.041589Z","iopub.status.busy":"2022-08-27T17:08:10.041193Z","iopub.status.idle":"2022-08-27T17:08:10.050020Z","shell.execute_reply":"2022-08-27T17:08:10.049344Z","shell.execute_reply.started":"2022-08-27T17:08:10.041548Z"},"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    def identity_loss(real_image, same_image, LAMBDA):\n","        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n","        return LAMBDA * 0.5 * loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-27T17:08:10.051503Z","iopub.status.busy":"2022-08-27T17:08:10.051059Z","iopub.status.idle":"2022-08-27T17:08:10.063583Z","shell.execute_reply":"2022-08-27T17:08:10.062773Z","shell.execute_reply.started":"2022-08-27T17:08:10.051468Z"},"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    monet_generator_optimizer = tf.keras.optimizers.Adam(6e-3)\n","    photo_generator_optimizer = tf.keras.optimizers.Adam(6e-3)\n","\n","    monet_discriminator_optimizer = tf.keras.optimizers.Adam()\n","    photo_discriminator_optimizer = tf.keras.optimizers.Adam()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-27T17:08:10.065183Z","iopub.status.busy":"2022-08-27T17:08:10.064684Z","iopub.status.idle":"2022-08-27T17:08:10.090048Z","shell.execute_reply":"2022-08-27T17:08:10.089149Z","shell.execute_reply.started":"2022-08-27T17:08:10.065143Z"},"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    cycle_gan_model = CycleGan(\n","        monet_generator, photo_generator, monet_discriminator, photo_discriminator\n","    )\n","\n","    cycle_gan_model.compile(\n","        m_gen_optimizer = monet_generator_optimizer,\n","        p_gen_optimizer = photo_generator_optimizer,\n","        m_disc_optimizer = monet_discriminator_optimizer,\n","        p_disc_optimizer = photo_discriminator_optimizer,\n","        gen_loss_fn = generator_loss,\n","        disc_loss_fn = discriminator_loss,\n","        cycle_loss_fn = calc_cycle_loss,\n","        identity_loss_fn = identity_loss\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-27T17:08:10.091639Z","iopub.status.busy":"2022-08-27T17:08:10.091186Z"},"trusted":true},"outputs":[],"source":["\n","cycle_gan_model.fit(\n","    tf.data.Dataset.zip((monet_ds, photo_ds)),\n","    epochs=40,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Visualize our Monet-esque photos"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["_, ax = plt.subplots(5, 2, figsize=(12, 12))\n","for i, img in enumerate(photo_ds.take(5)):\n","    prediction = monet_generator(img, training=False)[0].numpy()\n","    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n","    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n","\n","    ax[i, 0].imshow(img)\n","    ax[i, 1].imshow(prediction)\n","    ax[i, 0].set_title(\"Input Photo\")\n","    ax[i, 1].set_title(\"Monet-esque\")\n","    ax[i, 0].axis(\"off\")\n","    ax[i, 1].axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Step 4 Results & analysis\n"]},{"cell_type":"markdown","metadata":{},"source":["We have the next output now!<br>\n","![Results](images\\img2.jpg)\n","\n","For comparison, these are results of default CycleGAN implementation:<br>\n","![Default results](images\\tutorial_results.jpg)\n","\n","To my mind, most of my results are better.\n","\n","So we have a bit better results and we have much faster training at the same time.<br>\n","\n","Suming up, it helped to reduce layer size, kernel count and kernel size, and increasing optimizer learning rate parameters(and increasing epochs almost 2 times).<br>\n"," It helped because we reduced the complexity of neural network, so we are able to train more epochs at lower time now.<br>\n","What did not help is reducing epochs to have better time. It did not help because the style effect of output images decreased a lot and training time was still high."]},{"cell_type":"markdown","metadata":{},"source":["let's submit the results using next code"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import PIL\n","! mkdir ../images"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["i = 1\n","for img in photo_ds:\n","    prediction = monet_generator(img, training=False)[0].numpy()\n","    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n","    im = PIL.Image.fromarray(prediction)\n","    im.save(\"../images/\" + str(i) + \".jpg\")\n","    i += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import shutil\n","shutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")"]},{"cell_type":"markdown","metadata":{},"source":["### Step 5 Conclusion\n","So now we can know that simplifying the architecture and increasing training iterations can help sometimes.<br>\n","I was pretty limited by computational capabilities this time, so one thing I try later is buying and using a good videocard to be able to use bigger architectures and train models faster.<br>I will be also able to try much bigger epoch count, therefore, to see much more styled output.<br>\n","Also we know that CycleGAN is by far not the most modern technology, so I should try some newer and more complex image generation methods later."]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"vscode":{"interpreter":{"hash":"b37e7b5720c7e95a7ef2887ea8ee4097d9aacc35e40cedeade494a9f08331383"}}},"nbformat":4,"nbformat_minor":4}
